# Unified Optimization Design Document: Caligo Exploration Suite

This document defines the architectural refactoring required to transform the Caligo exploration pipeline into a **Data-Oriented, Zero-Copy, Streaming Architecture**. By integrating memory reduction strategies, strict type quantization, and inter-process shared memory, we aim to eliminate serialization overhead, maximize CPU cache locality via SIMD, and reduce RAM usage to a constant footprint regardless of campaign size.

---

## 1. The High-Performance Vector Foundation
**Target Modules:** `types.py`, `sampler.py`, `lhs_executor.py`

### The Architectural Shift
The foundation of the optimization is a move from loose, dynamic Python objects to strict, contiguous, lower-precision memory blocks. We will enforce **32-bit Quantization** (Float32) globally. This reduces the memory footprint by 50% and doubles the throughput of vectorized CPU instructions (AVX/SIMD), as modern processors can operate on twice as many 32-bit numbers per clock cycle as 64-bit numbers.

### Required Modifications

**1. Quantized Raw Memory Representation**
*   **Current State:** The system uses Python's standard `float` (64-bit double precision) and wraps data in `ExplorationSample` objects immediately.
*   **Modification:** The `LHSSampler` must be refactored to generate and store the design matrix solely as a `numpy.ndarray` with `dtype=numpy.float32`. All downstream transformations (log-uniform scaling, etc.) must enforce this data type to ensure the memory layout remains compact and contiguous from birth.

**2. Numba-Accelerated Feasibility Kernel**
*   **Current State:** Feasibility checks are performed sequentially on objects using the Python interpreter.
*   **Modification:** Implement the feasibility logic as a standalone kernel decorated with Numba JIT (Just-In-Time) compilation. Crucially, this kernel will be configured with `fastmath=True` and typed to accept `float32` arrays. This allows the compiler to generate highly optimized LLVM machine code that processes the feasibility of millions of samples in parallel using SIMD lanes, reducing the "search" phase latency to negligible levels.

**3. Just-In-Time Object Inflation**
*   **Current State:** Massive lists of objects are pre-generated.
*   **Modification:** The system will retain the raw `float32` matrix for the lifecycle of the campaign. `ExplorationSample` objects—which are required for the high-level protocol logic—must only be instantiated immediately prior to execution and discarded immediately after. This strictly decouples the mathematical exploration space (cheap) from the simulation runtime environment (expensive).

---

## 2. Zero-Copy Inter-Process Communication (IPC)
**Target Modules:** `epr_batcher.py`, `harness.py`

### The Architectural Shift
We will replace the standard Python multiprocessing queues (which rely on pickling/copying data) with a **Shared Memory** architecture. This eliminates the "serialization tax"—the CPU cost of converting arrays to bytes—and prevents memory spikes caused by duplicating massive quantum measurement arrays across process boundaries.

### Required Modifications

**1. Shared Memory Arena Allocation**
*   **Current State:** Each worker process generates private data and returns it to the main process via a pipe, causing a memory copy.
*   **Modification:** The Orchestrator must pre-allocate a global `multiprocessing.shared_memory.SharedMemory` block (the "Arena") capable of holding the measurement data for the active worker count. This memory block will be structured as a flat binary buffer.

**2. Zero-Copy Worker Write**
*   **Current State:** Workers return dictionary objects containing lists or arrays.
*   **Modification:** Workers will attach to the pre-allocated Shared Memory Arena. Instead of returning data, they will write their measurement outcomes (as `float32` or `int8`) directly into calculated offsets within the shared buffer. The worker then returns a lightweight metadata "pointer" containing only the batch ID and the buffer offset index.

**3. Zero-Copy Harness Read**
*   **Current State:** The main process unpickles the received data into new memory.
*   **Modification:** The Protocol Harness in the main process receives the metadata pointer and creates a NumPy array view directly over the existing Shared Memory block. This allows the simulation to read the quantum states generated by the worker without a single byte being copied in RAM.

---

## 3. Intelligent Streaming Execution with Guardrails
**Target Modules:** `active_executor.py`, `surrogate.py`, `lhs_executor.py`

### The Architectural Shift
We transition from a blind "Batch-and-Execute" model to an intelligent, filtered pipeline. By inserting a lightweight algorithmic guardrail before the heavy simulation, and utilizing streaming execution, we ensure the CPU never wastes cycles on doomed samples or idles while waiting for batch completion.

### Required Modifications

**1. The "Fail-Fast" Surrogate Guardrail**
*   **Current State:** Any sample deemed "theoretically feasible" by the simple mathematical check is sent to the expensive protocol simulation (taking minutes).
*   **Modification:** Implement a lightweight binary classifier (e.g., Random Forest) that runs alongside the heavy Gaussian Process. Before requesting EPR data or starting a simulation, the system must query this Guardrail. If the Guardrail predicts with high confidence that the Key Rate will be zero (even if theoretically feasible), the sample is "short-circuited." It is immediately marked as a failure with zero efficiency, bypassing the EPR generation and Protocol Harness entirely. This acts as a hyper-fast filter for complex failure modes.

**2. Generator-Based Pipelining**
*   **Current State:** Batch generation creates synchronization barriers where the system waits for all workers to finish.
*   **Modification:** The execution flow must use Python Generators to yield results the moment a worker completes a task (via `as_completed`). This allows the main process to begin the Guardrail check or Protocol simulation for `Sample A` while the workers are still generating quantum data for `Sample B`.

**3. Explicit Memory Lifecycle Management**
*   **Current State:** Memory cleanup relies on Python's garbage collector, which can be lazy.
*   **Modification:** Because we are using Shared Memory, the lifecycle must be explicit. Once a specific protocol execution completes (or is skipped by the Guardrail), the system must immediately flag that segment of the Shared Memory Arena as "free" for the next worker to use. This ensures the memory footprint remains constant and tied strictly to the number of CPU cores, not the number of samples.

---

## 4. Data-Oriented Persistence
**Target Modules:** `persistence.py`

### The Architectural Shift
The I/O layer must mirror the memory layout of the processing layer. By enforcing `Float32` persistence and pre-allocation, we eliminate data transformation overhead during save operations.

### Required Modifications

**1. Pre-Allocated Float32 Buffers**
*   **Current State:** Results are accumulated in dynamic lists, then converted to arrays for saving.
*   **Modification:** The HDF5 Writer class must pre-allocate fixed-size NumPy arrays of `dtype=float32` matching the batch size. As results stream out of the execution pipeline, data is inserted directly into these buffers.

**2. Direct HDF5 Alignment**
*   **Current State:** HDF5 datasets are generic.
*   **Modification:** The HDF5 datasets must be initialized explicitly with `dtype='float32'`. This ensures that flushing the memory buffer to disk is a raw binary dump (a direct memory transfer) rather than a value-by-value conversion process.

---

## 5. Algorithmic "Fail-Fast" Pruning (The Surrogate Guardrail)
**Target Modules:** `surrogate.py`, `active_executor.py`, `types.py`

### The Architectural Shift
We will transition the execution pipeline from a **deterministic** model (where every theoretically feasible point is simulated) to a **probabilistic** model. By introducing a hierarchical modeling stage, we decouple the task of *predicting feasibility* from the task of *predicting efficiency*. This creates a "short-circuit" mechanism that filters out complex failure modes—those that satisfy the mathematical bounds but fail the protocol due to specific noise or dynamic conditions—at a computational cost of microseconds rather than minutes.

### Required Modifications

**1. Dual-Model Surrogate Architecture**
*   **Current State:** The system uses a single Regressor (Gaussian Process) to predict efficiency. It relies on the simulator to discover points that result in zero key rate (failures).
*   **Modification:** Extend the `EfficiencyLandscape` to house two distinct models:
    1.  The existing **Regressor** (GP) for predicting continuous efficiency values.
    2.  A new, lightweight **Classifier** (e.g., Random Forest or a shallow Neural Network) trained on binary outcomes (Success/Failure).
    This Classifier acts as the "Guardrail." During the training phase (Phase 2/3), it consumes the same dataset as the GP but treats all outcomes with `key_rate > 0` as `1` and `key_rate == 0` as `0`.

**2. The Pre-Flight Inference Check**
*   **Current State:** The Executor sends every candidate batch directly to the EPR Generator and Protocol Harness.
*   **Modification:** Inject an inference step immediately before the EPR generation trigger. For every candidate sample, query the Guardrail Classifier.
    *   If the probability of failure exceeds a strict threshold (e.g., 99%), the sample is "pruned."
    *   Crucially, this pruning must be **opaque to the Optimizer** but **visible to the Dataset**. The Executor must generate a synthetic `ProtocolResult` with `outcome=SKIPPED_PREDICTED_FAILURE` and `efficiency=0.0`.
*   **Performance Gain:** This prevents the simulation of "dead zones" in the parameter space. The system avoids allocating shared memory, spawning processes, or running quantum simulations for points that have virtually no chance of success.

**3. Imputed Training Feedback Loop**
*   **Current State:** The GP is only trained on data that comes back from the harness.
*   **Modification:** The "pruned" results must be fed back into the Gaussian Process training set as valid data points with `y=0`. This ensures that the GP learns that the pruned region is indeed low-efficiency, preventing the Bayesian Optimizer from repeatedly suggesting points in that area. This creates a self-reinforcing loop where the Guardrail protects the Simulator, and the "virtual" data protects the Guardrail from drift.

---

Here is the refined and extended **Section 6**, technically specified for **GPyTorch** integration as the chosen backend.

---

Here is the final, comprehensive version of **Section 6** for the design document.

---

## 6. GPU-Accelerated Linear Algebra for Surrogates
**Target Modules:** `surrogate.py`, `active.py`, `active_executor.py`

### The Architectural Shift
To handle the computational complexity of Gaussian Processes (GPs)—which naively scales cubically, $O(N^3)$, with the number of samples—we will migrate the surrogate modeling layer from CPU-bound Scikit-Learn to a fully GPU-accelerated stack using **GPyTorch** and **BoTorch**.

Current CPU implementations rely on dense Cholesky decompositions via OpenBLAS/MKL, which become computationally intractable and memory-prohibitive beyond $N \approx 5,000$ samples. By shifting to GPyTorch, we leverage **Black Box Matrix-Matrix (BBMM)** multiplication and Preconditioned Conjugate Gradient (PCG) solvers. This reduces the algorithmic complexity to $O(N^2)$ or even $O(N)$ with symbolic backends, enabling the system to scale efficiently to $N=100,000+$ samples while keeping the "Think" phase of the Active Learning loop in the sub-second range.

### Required Modifications

**1. GPyTorch Model Integration**
*   **Current State:** The `EfficiencyLandscape` wrapper delegates to `sklearn.gaussian_process.GaussianProcessRegressor`, managing kernel state and executing fitting via `scipy.optimize` on the CPU.
*   **Modification:** Replace the Scikit-Learn wrapper with a custom class inheriting from `gpytorch.models.ExactGP`.
    *   **Kernel Definition:** Implement a specialized Automatic Relevance Determination (ARD) kernel structure: a `ScaleKernel` wrapping a `MaternKernel` (with $\nu=2.5$ and 9 distinct length scales). This strictly enforces the smoothness assumptions required for the QKD parameter space while allowing the model to learn feature importance automatically.
    *   **Training Loop:** Replace the atomic `fit()` call with an explicit PyTorch optimization loop. The Executors must instantiate a `torch.optim.Adam` optimizer and minimize the `gpytorch.mlls.ExactMarginalLogLikelihood` loss.
    *   **Algorithmic Solver:** Explicitly configure the GPyTorch settings to force the usage of **CG/Lanczos solvers** (`max_cholesky_size(0)`). This ensures the system avoids dense matrix factorization entirely, relying instead on iterative matrix-vector multiplications that are highly parallelizable on GPUs.

**2. Strict Float32 Quantization & Jitter Control**
*   **Current State:** Scikit-Learn defaults to `float64` (Double Precision), which consumes double the VRAM and cannot utilize the high-throughput Tensor Cores found on modern NVIDIA GPUs.
*   **Modification:** The system must enforce a strict `torch.float32` pipeline.
    *   **Data Transfer:** Input arrays from the HDF5 persistence layer must be immediately transferred to VRAM and cast using `torch.tensor(..., dtype=torch.float32, device='cuda')`.
    *   **Model Casting:** The GP model itself must be cast to single precision immediately after instantiation.
    *   **Numerical Stability:** To prevent non-positive definite errors in the covariance matrix—common when degrading precision—the diagonal jitter (`gpytorch.settings.cholesky_jitter`) must be tuned (likely increased from default $10^{-6}$ to $10^{-5}$). This ensures numerical stability without reverting to the memory-heavy `float64`.

**3. BoTorch-Driven Active Learning**
*   **Current State:** The `BayesianOptimizer` uses `scipy.optimize` (CPU) to maximize acquisition functions defined in Numba. This requires moving posterior predictions back to host RAM for every function evaluation, creating a bottleneck and preventing the use of gradient-based optimization.
*   **Modification:** Replace the manual optimization logic with **BoTorch**, which is built on top of GPyTorch.
    *   **Custom Acquisition:** Implement the "Straddle" heuristic (identifying the zero-efficiency cliff) as a custom `botorch.acquisition.AcquisitionFunction`. This allows the heuristic to be differentiable with respect to the inputs.
    *   **Gradient-Based Optimization:** Use BoTorch’s `optimize_acqf` to find candidate points via gradient ascent (using Autograd) directly on the GPU. This is significantly faster and more accurate than derivative-free random search in high-dimensional spaces.
    *   **Joint q-Batch Parallelism:** Instead of iterative heuristics like "Kriging Believer," utilize BoTorch’s ability to optimize a joint batch of $q$ candidates simultaneously. This accounts for the correlation between selected candidates, producing a more diverse batch for the parallel workers in a single optimization pass.

**4. Scalable Memory Management (VRAM Optimization)**
*   **Current State:** Not applicable (CPU-bound).
*   **Modification:** Implement specific fail-safes to prevent Out-Of-Memory (OOM) errors as the dataset grows.
    *   **Symbolic Kernel Matrices (KeOps):** Integrate `pykeops` as the kernel backend. When enabled, this performs symbolic reductions for the kernel matrix without ever materializing the full $N \times N$ tensor in VRAM. This reduces the memory complexity from quadratic $O(N^2)$ to linear $O(N)$, theoretically allowing millions of samples on a standard GPU.
    *   **Gradient Checkpointing:** Enable `torch.utils.checkpoint` during the GP training loop. This trades computation for memory by discarding intermediate activation layers during the forward pass and recomputing them during the backward pass, reducing peak training memory by approximately 50-60%.
    *   **Inference Partitioning:** In the acquisition phase, if the pool of candidates for discrete evaluation exceeds VRAM limits, the prediction method must automatically chunk the input tensor (e.g., splitting 50,000 candidates into 5 batches of 10,000), accumulating the resulting mean and variance tensors before passing them to the acquisition function.

---

## 7. Test Suite Adaptation for High-Performance Architecture
**Target Modules:** `test_types.py`, `test_sampler.py`, `test_persistence.py`, `test_pipeline_integration.py`

### The Architectural Shift in Testing
The transition to a data-oriented, quantized, zero-copy architecture necessitates a corresponding shift in the verification strategy. Tests that previously asserted object equality (e.g., `ExplorationSample` == `ExplorationSample`) must now assert numerical equivalence on memory buffers (e.g., `numpy.allclose`). Furthermore, the testing framework must validate the structural integrity of the new shared memory and guardrail components to prevent silent regression errors in the optimized pipeline.

### Required Modifications

**1. Quantization & Vectorization Verification (`test_sampler.py`, `test_types.py`)**
*   **Precision Tests:** Update all `ExplorationSample` and `ProtocolResult` tests to enforce `dtype=numpy.float32`. Tests must explicitly verify that data returned from the sampler and stored in HDF5 does *not* contain 64-bit floats, which would break the optimization chain.
*   **Vectorization Benchmarks:** Introduce performance regression tests for the new `_check_feasibility_kernel`. These tests should measure the execution time of processing $10^5$ samples and fail if the throughput drops below a defined threshold (e.g., 5ms), ensuring that Numba compilation is correctly triggering SIMD instructions.

**2. Shared Memory Integrity Tests (`test_thread_safety.py`)**
*   **Lifecycle assertions:** Create new tests that simulate the full lifecycle of a shared memory block: `Allocation -> Worker Write -> Harness Read -> Deallocation`. These tests must verify that:
    *   Data written by a child process is bit-exact when read by the parent.
    *   No "zombie" shared memory segments remain in `/dev/shm` after the test fixture tears down (crucial for CI stability).
*   **Concurrency Stress:** Simulate multiple workers writing to adjacent slots in the Shared Memory Arena simultaneously to ensure no memory corruption or race conditions occur at slot boundaries.

**3. Guardrail Logic Verification (`test_surrogate.py`)**
*   **Pruning Accuracy:** Add tests for the new Classifier-based guardrail. These should feed known "dead" samples (e.g., extremely high loss parameters) and assert that the system returns a `SKIPPED_PREDICTED_FAILURE` outcome *without* invoking the mock protocol harness.
*   **Pipeline Bypass:** Verify that when a sample is pruned, the `BatchedEPROrchestrator` is *not* called, confirming the computational savings.

**4. End-to-End Pipeline Updates (`test_pipeline_integration.py`)**
*   **Zero-Copy Flow:** Update integration tests to use the new `Generator`-based execution flow instead of the list-based batch flow. The tests must consume the generator and verify that results are yielded incrementally.
*   **GPU Mocking:** Since CI environments may lack GPUs, introduce a `MockGPUBackend` fixture for `test_active.py` that mimics the interface of the new GPU-accelerated Linear Algebra module but performs calculations on the CPU. This allows logic verification of the "VRAM Transfer" and "Batched Acquisition" steps without requiring hardware hardware.

---

## 8. `main_explor.py` Recommended Modifications

While logically sound, the script assumes a "happy path" regarding system resources and file states. To make it a production-grade orchestrator—especially given the massive memory optimizations discussed in previous steps—I recommend the following extensions:

#### 1. Explicit Resource Flushing (Garbage Collection)
The pipeline runs in a single Python process. While `executor._cleanup()` is called, Python's Garbage Collector (GC) is lazy. Large structures (like the Phase 1 LHS buffer or Phase 2 training arrays) might linger in RAM during Phase 3, causing Out-Of-Memory (OOM) errors.

**Modification:** Force GC between phases.
```python
import gc

# ... after run_phase1 ...
del metrics1, executor
gc.collect()  # Force release of LHS buffers
logger.info("Garbage collection complete after Phase 1")

# ... after run_phase2 ...
del metrics2, executor
gc.collect()  # Force release of training tensors
```

#### 2. "Pre-Flight" Existence Checks
If a user runs with `--skip-phase1`, the script assumes `exploration_data.h5` exists. If it doesn't, the script will initialize Phase 2 and crash with a confusing stack trace inside the executor.

**Modification:** Fail fast in the `main` function.
```python
if args.skip_phase1 and not (config.output_dir / "exploration_data.h5").exists():
    logger.error("Cannot skip Phase 1: 'exploration_data.h5' not found.")
    return 1

if args.skip_phase2 and not (config.output_dir / "surrogate.pkl").exists():
    logger.error("Cannot skip Phase 2: 'surrogate.pkl' not found.")
    return 1
```

#### 3. Global Seed Re-Assertion
While `CampaignConfig` holds the seed, global state in libraries (like `numpy.random` or `netsquid`) might drift or be altered by internal calls in previous phases. To ensure that running "Phase 3 only" yields the *exact same behavior* as running "Phase 1 then 2 then 3", re-seed before every phase.

**Modification:**
```python
# Before Phase 2
np.random.seed(config.execution['random_seed'])
run_phase2(config)

# Before Phase 3
np.random.seed(config.execution['random_seed'])
run_phase3(config)
```

#### 4. Shared Memory Cleanup Trap
If the pipeline is updated to use the **Shared Memory IPC** (from the optimization design), a crash in `main_explor.py` could leave "zombie" shared memory blocks in the OS (`/dev/shm`), effectively leaking RAM until a reboot.

**Modification:** Wrap the entire execution in a global exception handler that ensures IPC cleanup.
```python
# In main()
from multiprocessing.shared_memory import SharedMemory

try:
    # ... execution ...
except Exception:
    # ... error logging ...
finally:
    # Emergency cleanup of any lingering shared memory blocks
    # tied to this campaign ID
    cleanup_shared_memory_orphans(campaign_id)
```