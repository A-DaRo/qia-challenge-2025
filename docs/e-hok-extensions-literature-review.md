# E-HOK Protocol Components and Foundational Literature

## Phase I: Physical Foundations & Security Assumptions

### Goal 1.1: The Quantum Memory Threat (Noisy-Storage Model)

**Functional role:** This phase ensures the protocol's security against a powerful quantum adversary by leveraging physical limitations of quantum memory. In E-HOK, Bob's ability to delay measurement on entangled qubits until Alice reveals her bases would break security. The **Noisy-Storage Model (NSM)** posits that Bob's quantum memory suffers decoherence over time, so he cannot perfectly store large quantum states for long delays. The critical condition to enforce is an entropic uncertainty bound, e.g.:

$$\gamma\Delta t > H_{\max}(K|E),$$

where $\gamma$ is Bob's decoherence rate, $\Delta t$ is the enforced waiting time, and $H_{\max}(K|E)$ is the max-entropy of Alice's raw key conditioned on Bob's information [1](./e-hok-extensions.md). Intuitively, this inequality means that the uncertainty (entropy) Bob has about Alice's key must exceed the product of noise and delay, forcing physical (thermodynamic) security rather than computational hashing.

**Foundational literature:** Wehner et al. introduced the NSM, showing that limited (noisy) quantum storage enables secure 1-2 OT: any sufficiently noisy storage channel forces Bob's guessing probability for both bases to be small [2](./literature/Cryptography%20from%20Noisy%20Storage.md). König et al. extended this with **entropic uncertainty proofs under arbitrary attacks** (beyond IID assumptions), proving unconditionally secure OT/BC with only NSM [3](./literature/Unconditional-security-from-noisy-quantum-storage.md). Ng et al. experimentally realized bit commitment under NSM, demonstrating feasibility with current optics and deriving practical bounds on memory parameters [4](./literature/Experimental%20implementation%20of%20bit%20commitment%20in%20the%20noisy-storage%20model.md) (Damgård et al.'s bounded-storage model is a related paradigm assuming size-limited memory, but NSM is strictly more general.) Key results include entropic uncertainty relations (e.g. for post-measurement information [3] ) and trade-off formulas between decoherence parameters and security epsilon.

**Implementation relevance:** The E-HOK SquidASM pipeline must simulate real-time delays and noisy memories. In practice, we calibrate the required $\Delta t$ for a given $T_1/T_2$ relaxation time so that $\gamma\Delta t$ exceeds $H_{\max}(K|E)$. Literature bounds (e.g. from König et al.) translate into lookup tables of $T_2$ vs. allowed key length and error rate. For example, Wehner et al. showed that even modest noise yields negligible mutual information for Bob 2; we incorporate similar bounds into parameter choices. In SquidASM, we model each idle qubit with amplitude-damping and dephasing channels (characterized by $T_1$ and $T_2$) and explicitly insert a `wait ($\Delta t$`) before revealing bases. We then empirically sweep Bob's success probability and verify that his cheating advantage drops below $\varepsilon$ when the **noisy-storage inequality** holds. In summary, Phase I ties directly to physical security proofs in the literature: it ensures our hardware and timing satisfy the NSM assumptions of Wehner/König and others [2], [3].

## Phase II: Core Protocol Logic (Sifting & Estimation)
### Goal 1.2: The Sampling Attack (Cut-and-Choose)

**Functional role:** This phase defines how Alice and Bob partition and test the raw key bits to estimate channel noise and catch cheating. Bob, as adversary, might measure “test” qubits honestly to pass Alice's check while cheating on "key" qubits. A naive random sample fails if Bob learns the sample indices in advance. Thus, E-HOK uses a **commit-then-challenge (cut-and-choose)** pattern: Bob first commits to all measurement outcomes, then Alice randomly (but unpredictably) chooses a subset of positions to open. This statistically bounds Bob's information on the remaining bits.

**Foundational literature:** In classical two-party computation, Lindell and Pinkas proved that **cut-and-choose** can enforce honest behavior in Yao's protocol: by randomly testing a subset of circuit evaluations, one can bound cheating with high probability [5](./literature/Extending%20Oblivious%20Transfers%20Efficiently.md). Fung et al. (QKD context) studied **mismatch-basis statistics**, showing how to account for biases when the adversary's strategies differ on test vs key bits (e.g. exploiting basis choice). Finally, Hoeffding's inequality underlies concentration: the probability that Bob's undetected cheating deviates from the observed test error decays as $\exp(-2|T|t^2)$ for sample size $|T|$ and deviation $t$. Formally, if $e_{\rm test}$ is the error in test bits, Hoeffding bounds ensure with high probability that the error on the untested key bits is close to $e_{\rm test}$.

**Implementation relevance:** The pipeline implements a **Merkle-tree or hash commitment** for Bob's outcomes, followed by a PRNG-driven challenge index. The literature informs how large the test set $|T|$ must be. For instance, if Hoeffding's bound $2\exp(-2|T|t^2)<\delta$, we pick $|T|$ so Bob's knowledge gap $t$ on key bits stays below $\delta$. We draw on Lindell-Pinkas to simulate many base OTs (Step A) and then sample-check them (Step B) [5]. In code, we ensure Bob locks in all measurement results before revealing any indices. We simulate malicious Bob strategies (e.g. block cheats vs random cheats) and verify that with cut-and-choose, **unmasking probability** (Alice catching Bob) matches the analytic predictions. In summary, Phase II's design is guided by classical "cut-and-choose” analysis [5] and QKD parameter estimation results, yielding algorithms to set the test fraction and abort thresholds.

## Phase III: Zero-Knowledge Reconciliation (Error Correction)
### Goal 2.1: Blind Reconciliation & Wiretap Coding

**Functional role:** This phase corrects Bob's errors on the sifted key ($I_0$) without leaking information on Alice's bit values. Conceptually it's a **wiretap channel problem**: Alice's LDPC syndrome must allow Bob to reconcile but should reveal minimal mutual information to a would-be eavesdropper. E-HOK uses **dual-graph MET-LDPC codes** (multi-edge type LDPC) that approach capacity. The reconciliation must be **one-shot** (low interaction) to fit high-throughput hardware, and adaptable to unknown error rates (hence "blind" reconciliation).

**Foundational literature:** Wyner's seminal **wiretap channel** work introduced the notion of **secrecy capacity**, showing that by encoding into cosets of a linear code, one can achieve secure message transmission when the eavesdropper's channel is noisier than the main channel. In practice, this means designing codes whose syndromes leak virtually no information. Martinez-Mateo et al. ("Blind Reconciliation") analyzed short-length LDPC for QKD, showing that $2\times 10^3$-bit LDPC codes can efficiently reconcile without prior error-rate estimation [6](./literature/Blind%20Reconciliation.md). They demonstrate that short, hardware-friendly LDPC codes yield high throughput in practice. Richardson et al. established the methodology of designing **irregular LDPC codes** near Shannon capacity via density evolution (we use their degree-profile techniques to optimize our MET-LDPC ensembles). Ye and Narayanan studied **secret-key extraction from correlated Gaussian sources**, providing continuous-variable analogues of wiretap coding (their formulas guide how much parity-check information can safely be revealed given a certain noise level).

**Implementation relevance:** We implement a **rate-adaptive LDPC module**: given observed bit error rate $e$, the code's rate is adjusted so that Bob's residual entropy is negligible. Using the blind-reconciliation insights [6], we avoid extra rounds to estimate $e$ and instead try multiple puncturing/shortening patterns in one pass. The literature implies constraints like $R + R_{\rm leak} \le 1 - h(e)$ (from Wyner's $C_{\rm sec} =I(X;Y)-I(X;Z)$), where $R_{\rm leak}$ is syndrome rate. Practically, we set up our encoder so that the LDPC syndrome reveals the right amount of information: enough for Bob to decode, but so that $I(Alice; Eve)$ (the eavesdropper's equivocation) is negligible. We verify that the final key length $\ell$ obeys typical finite-key formulas (see next section). In summary, Phase III integrates wiretap coding theory with practical LDPC design 8: we build a reconciler that is both hardware-efficient (short LDPC) and provably privacy-preserving (coset coding ensures secrecy as per Wyner's wiretap channel).

## Phase IV: Statistical Rigor & Composability
### Goal 4.1: Finite-Key Analysis

**Functional role:** With realistic (finite) number of qubits, we must quantify statistical fluctuations. This yields the final key length formula and security bounds. We compute the **smooth min-entropy** of Alice's key conditioned on any adversary, subtract leakage, and apply finite-size corrections $\Delta(\epsilon,N)$.

**Foundational literature:** Tomamichel et al. provided a **tight finite-key bound** for QKD: security can be proven for moderate block sizes $M$ using entropic uncertainty relations for smooth entropies [7](./literature/Tight%20Finite-Key%20Analysis%20for%20Quantum%20Cryptography.md). They show the asymptotic $N(1-h(e))$ keyrate acquires a $-\Delta$ correction (typically $O(\sqrt{N})$) for finite $N$. Renner & Wolf defined **smooth Rényi entropies** $H_{\min}^\epsilon$ and $H_{\max}^\epsilon$ as fundamental measures of extractable randomness [8](./literature/Smooth%20Renyi%20Entropy%20and%20Applications.md). For example, they prove $H_{\min}^\epsilon(Z) \ge H_\alpha(Z) - \frac{\log(1/\epsilon)}{\alpha-1}$ 10, which underlies finite-key security: as $N\to\infty$, $H_{\min}\to N H(Z)$, but for finite $N$ an extra smoothing gap of order $\log(1/\epsilon)$ appears. Damgård et al. (Crypto'14) showed how **commit-and-open techniques** can tighten finite-key analysis of bit commitment/OT, effectively improving the bound on $\Delta$.

**Implementation relevance:** We incorporate these results into our Pipeline by setting the key length $\ell$ according to e.g. Tomamichel's formula:

$$\ell\le N [1-h(e_{\rm key})] – \text{leak}_{\rm EC} – \Delta(\epsilon, N),$$

where $\text{leak}_{\rm EC}$ accounts for parity bits disclosed, and $\Delta$ covers statistical confidence [7], [8]. In practice we run Monte Carlo sampling of hypothetical adversary bias and verify that our chosen $N$ and test fraction yield final security $\epsilon$ as in Renner-Wolf [8]. In sum, Phase IV translates the abstract smooth-entropy bounds into concrete block sizes and error tolerances in the implementation, ensuring our finite-$N$ key is indistinguishable from random up to $\epsilon$. We store lookup tables of $\Delta(N,e,\epsilon)$ so that the SquidASM pipeline can quickly determine how large $N$ must be for desired security. Throughout, we ensure composable security by treating $H_{\min}$

### Goal 3.1: Universal Composability (OT Extension)

**Functional role:** E-HOK is a hybrid protocol that seeds classical **OT extension**. We must ensure keys from E-HOK behave like ideal base-OT correlations in any larger cryptographic protocol. In the UC framework, this means no subtle correlations or side-information leak. This phase involves OT Extension and UC security analysis.

**Foundational literature:** Ishai et al. (IKNP03) introduced **OT extension**: using $k$ base OTs one can generate arbitrarily many (say $n$) OTs with only symmetric operations [5]. They show that $n\gg k$ at constant amortized cost: e.g. **128 base OTs suffice for millions of OTs** (IKNP). Boyle et al. (CCS'19) further improve this with **silent preprocessing**: a 2-round OT extension where only a small input-independent setup is done online, matching the "low bandwidth, high latency" nature of quantum links [9](./literature/Efficient%20Two-Round%20OT%20Extension%20and%20Silent%20Non-Interactive%20Secure%20Computation.md). Boyle's protocol is concretely efficient and even secure in the malicious setting without extra interaction. Canetti's **UC framework** provides the definitions ("Keyed functionality is indistinguishable from ideal OT" [1]), and Unruh (Eurocrypt'10) extended UC security to hybrid quantum-classical protocols.

**Implementation relevance:** Practically, we model E-HOK's output as **128 base OTs** (Alice's key bits vs Bob's measurement results). We then implement the IKNP03 OT-extension algorithm in SquidASM (or Python) as a test: using the 128 "quantum OT" values to generate $10^6$ extended OTs. We then statistically compare the resulting OT output distribution to a true random OT (distinguisher test). The literature instructs that the **statistical distance** (e.g. Kolmogorov distance) must be below $\epsilon_{\rm UC}$ [1](./e-hok-extensions.md). In parallel, we analyze how many base OTs per second are needed to sustain a target classical throughput: the “key refresh rate" comes from IKNP's amortization and Boyle's 2-round insights. For example, Boyle et al. show the first efficient 2-round OT extension for semi-honest (and later malicious) security [9]; we adopt their formulas to model latency vs. throughput. In summary, Phase IV/V ensures that when E-HOK is composed with classical OT extension (IKNP or silent OT), the joint protocol remains UC-secure, leveraging the established OT-extension theory [5] [9].

## Phase V: Advanced Architecture (Star Topology)
### Goal 4.2: MDI (Measurement-Device-Independent) Architecture

**Functional role:** This optional phase redesigns E-HOK to a **measurement-device-independent (MDI)** style to eliminate detector side-channel attacks. We introduce a central untrusted node (Charlie) who performs a **Bell-state measurement (BSM)** on qubits sent by Alice and Bob. Alice and Bob never reveal individual outcomes; Charlie's announcements create correlations akin to entanglement swapping. The goal is that Charlie learns nothing about Alice's bit $I_1$ selections (the oblivious key) even if he manipulates detectors.

**Foundational literature:** Lo et al.'s MDI-QKD proposal showed that by using an untrusted relay, one removes all detector vulnerabilities, achieving security as long as Alice and Bob trust their state preparation. Pirandola et al. later extended MDI to **continuous-variable MDI-QKD**, achieving extremely high key rates (three orders of magnitude above standard MDI) [10](./literature/High-rate%20measurement-device-independent%20quantum%20cryptography.md). Lucamarini et al. analyzed the **Trojan-horse attack** (bright-light injection) and provided security bounds: they recast TH into an information-leak problem, deriving quantitative reflector/transmissivity specs [11](./literature/Practical%20security%20bounds%20against%20the%20Trojan-horse%20attack%20in%20quantum%20key%20distribution.md). (MDI-QT – measurement-device-independent quantum transfer/OT – is an emerging research area with few results; it is effectively a “high-value gap" in literature.)

**Implementation relevance:** In SquidASM, we configure a 3-node network: Alice and Bob each connect to Charlie. Charlie's quantum processor performs a BSM (e.g. on incoming polarization qubits). We simulate honest Charlie (announcing correct Bell outcomes) and malicious Charlie (random/fraudulent announcements). The sampling phase (Phase II) must catch inconsistencies if Charlie lies. For example, if Charlie announces a result inconsistent with Alice/Bob's bases, Alice and Bob detect excessive error and abort. We adapt Lucamarini's TH bounds by specifying optical isolation: e.g. limit any injected photon number. From Pirandola's result [10], we expect (and test) that using coherent-state encodings yields high rates even in MDI mode. Crucially, even if Charlie is malicious, the protocol structure (Alice and Bob never send complementary information to Charlie) ensures he gains no information about $I_1$. In summary, Phase V leverages **MDI-QKD theory** [10] to architect a detector-independent E-HOK, relying on Charlie's honest-but-untrusted measurement model. We implement it by extending the network topology in `network_config.yaml`, adding a Bell-state measurement module, and verifying (via Monte Carlo) that no side-channel attack (blinding, Trojan-horse) can compromise the key beyond the bounds in the literature [11], [10].

**Equations & Bounds:** Among the key mathematical results invoked are entropic uncertainty relations (bounding $H_{\max}(K|E)$ under noise [3]), sampling bounds ($\Pr[\text{error}] \le e^{-2|T|t^2}$ by Hoeffding), and finite-key corrections (Tomamichel's $\Delta(N,\epsilon)$). For example, the one-sided smooth min-entropy bound $H_{\min}^\epsilon \gtrsim H_{\alpha} - \frac{\log(1/\epsilon)}{\alpha-1}$ [8] quantifies how much key length is lost to ensure $\epsilon$-security. Practically, our code uses such bounds (translated from the cited works) to set error margins.

**Summary:** Each protocol component of E-HOK directly builds on foundational quantum-crypto results. Phase I's security comes from the noisy-storage trade-offs proven by Wehner (PRL 2008) and König (TIT 2012) [2], [3]. Phase II's cut-and-choose sampling is justified by Lindell-Pinkas and QKD parameter-estimation theory [5]. Phase III's blind reconciliation relies on wiretap coding (Wyner 1975) and practical LDPC designs [6]. Phase IV's finite-key math is drawn from Tomamichel-Renner et al. [7], [8]. Finally, Phase V's composability and MDI design cite standard OT-extension protocols [5] [9] and MDI-QKD/MDI-QC analyses [10], [11]. In implementation, we translate these theoretical insights into SquidASM modules (noise models, commit/challenge logic, LDPC encoder, statistical check routines, and multi-node networks), ensuring the E-HOK pipeline rigorously reflects its literature foundations.


## References: 

[1] `./e-hok-extensions.md`

[2] `./literature/Cryptography from Noisy Storage.md` (https://staff.science.uva.nl/c.schaffner/mypapers/WST08.pdf)

[3] `./literature/Unconditional-security-from-noisy-quantum-storage.md` (https://qutech.nl/wp-content/uploads/2017/03/Unconditional-security-from-noisy-quantum-storage-1.pdf)

[4] `./literature/Experimental implementation of bit commitment in the noisy-storage model.md` (https://arxiv.org/abs/1205.3331)

[5] `./literature/Extending Oblivious Transfers Efficiently.md` (https://web.engr.oregonstate.edu/~rosulekm/scbib/index.php?n=Paper.IKNP03)

[6] `./literature/Blind Reconciliation.md` (https://arxiv.org/abs/1205.5729)

[7] `./literature/Tight Finite-Key Analysis for Quantum Cryptography.md` (https://arxiv.org/abs/1103.4130)

[8] `./literature/Smooth Renyi Entropy and Applications.md` (https://crypto.ethz.ch/publications/files/RenWol04a.pdf)

[9] `./literature/Efficient Two-Round OT Extension and Silent Non-Interactive Secure Computation.md` (https://pure.au.dk/portal/en/publications/efficient-two-round-ot-extension-and-silent-non-interactive-secur/)

[10] `./literature/High-rate measurement-device-independent quantum cryptography.md` (https://www.nature.com/articles/nphoton.2015.83?error=cookies_not_supported&code=7546ac46-ca97-4321-8d21-fb0364e7679d)

[11] `./literature/Practical security bounds against the Trojan-horse attack in quantum key distribution.md` (https://arxiv.org/abs/1506.01989)